{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the FSMol Dataset\n",
    "\n",
    "The `FSMolDataset` class provides access to the train/valid/test tasks of our few-shot learning dataset on molecules.\n",
    "\n",
    "An instance is created from the data directory by `FSMolDataset.from_directory(/path/to/dataset)`.\n",
    "The data in the `train`, `validation` and `test` folds of the dataset can be accessed using `FSMolDataset.get_task_reading_iterable`.\n",
    "By default, this method simply reads in the individual data files using a number of background worker processes and provides them in a standard format, but this behavior can be customized by providing a specialized callback function.\n",
    "The default implementation returns an iterable over `FSMolTask` objects, each containing an entire task's of single featurised molecules, `MoleculeDatapoint`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up local details:\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# This should be the location of the checkout of the FS-Mol repository:\n",
    "FS_MOL_CHECKOUT_PATH = os.path.join(\"/data/local/apps/\", \"Meta-Learning\", \"FS-Mol\")\n",
    "FS_MOL_DATASET_PATH = os.path.join(\"/data/local/apps/\", \"Meta-Learning\", \"FS-Mol\", \"datasets\")\n",
    "\n",
    "os.chdir(FS_MOL_CHECKOUT_PATH)\n",
    "sys.path.insert(0, FS_MOL_CHECKOUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Creating FSMolDataset and Accessing a Task\n",
    "\n",
    "First, we simply create a dataset and have a look at a single task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHEMBL1243966\n"
     ]
    }
   ],
   "source": [
    "from fs_mol.data import FSMolDataset, DataFold\n",
    "dataset = FSMolDataset.from_directory(FS_MOL_DATASET_PATH)\n",
    "\n",
    "valid_task_iterable = dataset.get_task_reading_iterable(DataFold.VALIDATION)\n",
    "task = next(iter(valid_task_iterable))\n",
    "print(task.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A task, by default, is simply a name and a list of datapoints, each stored as a `MoleculeDatapoint`, a dataclass with the following definition:\n",
    "\n",
    "```python\n",
    "@dataclass(frozen=True)\n",
    "class MoleculeDatapoint:\n",
    "    \"\"\"Data structure holding information for a single molecule.\n",
    "\n",
    "    Args:\n",
    "        task_name: String describing the task this datapoint is taken from.\n",
    "        smiles: SMILES string describing the molecule this datapoint corresponds to.\n",
    "        graph: GraphData object containing information about the molecule in graph representation\n",
    "            form, according to featurization chosen in preprocessing.\n",
    "        numeric_label: numerical label (e.g., activity), usually measured in the lab\n",
    "        bool_label: bool classification label, usually derived from the numeric label using a\n",
    "            threshold.\n",
    "        fingerprint: optional ECFP for the molecule.\n",
    "        descriptors: optional phys-chem descriptors for the molecule.\n",
    "    \"\"\"\n",
    "\n",
    "    task_name: str\n",
    "    smiles: str\n",
    "    graph: GraphData\n",
    "    numeric_label: float\n",
    "    bool_label: bool\n",
    "    fingerprint: Optional[np.ndarray]\n",
    "    descriptors: Optional[np.ndarray]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MoleculeDatapoint(task_name='CHEMBL1243966', smiles='CC(C)n1nc(-c2cc(O)cc(Br)c2)c2c(N)ncnc21', graph=GraphData(node_features=array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.]],\n",
       "      dtype=float32), adjacency_lists=[array([[ 0,  1],\n",
       "       [ 1,  2],\n",
       "       [ 1,  3],\n",
       "       [ 3,  4],\n",
       "       [ 5,  6],\n",
       "       [ 7,  8],\n",
       "       [ 8,  9],\n",
       "       [10, 11],\n",
       "       [11, 12],\n",
       "       [ 5, 14],\n",
       "       [15, 16],\n",
       "       [15, 17],\n",
       "       [18, 19],\n",
       "       [20,  3],\n",
       "       [13,  6],\n",
       "       [20, 14]]), array([[ 4,  5],\n",
       "       [ 6,  7],\n",
       "       [ 8, 10],\n",
       "       [11, 13],\n",
       "       [14, 15],\n",
       "       [17, 18],\n",
       "       [19, 20]]), array([], shape=(0, 2), dtype=int64)], edge_features=[]), numeric_label=255.0, bool_label=True, fingerprint=array([0, 1, 0, ..., 0, 0, 0], dtype=int32), descriptors=array([ 9.7854939e+00,  1.3546285e-01,  9.7854939e+00,  1.3546285e-01,\n",
       "        7.4265379e-01,  3.4820401e+02,  3.3409201e+02,  3.4703818e+02,\n",
       "        1.0800000e+02,  0.0000000e+00,  1.6371268e-01, -5.0790083e-01,\n",
       "        5.0790083e-01,  1.6371268e-01,  1.2380953e+00,  2.0000000e+00,\n",
       "        2.6666667e+00,  2.3672352e+00,  8.0733759e+02,  1.5145900e+01,\n",
       "        1.1578135e+01,  1.3164130e+01,  9.9692345e+00,  6.4570394e+00,\n",
       "        7.2500377e+00,  5.0425453e+00,  5.9582205e+00,  3.1632545e+00,\n",
       "        3.6210921e+00,  2.2683961e+00,  2.7495639e+00, -2.1500001e+00,\n",
       "        6.8506633e+04,  1.3815788e+01,  4.9959621e+00,  2.3321557e+00,\n",
       "        1.2931319e+02,  1.0840195e+01,  2.3588623e+01,  5.6471772e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        1.4649760e+01,  5.0986819e+00,  0.0000000e+00,  1.5929944e+01,\n",
       "        3.2046577e+01,  1.6078012e+01,  5.3862243e+00,  5.1065273e+00,\n",
       "        3.2781208e+01,  0.0000000e+00,  1.9748442e+01,  0.0000000e+00,\n",
       "        1.9889315e+01,  5.7336674e+00,  2.8999142e+01,  0.0000000e+00,\n",
       "        1.7006891e+01,  5.7336674e+00,  5.8178630e+00,  5.7495117e+00,\n",
       "        1.5929944e+01,  2.4854969e+01,  0.0000000e+00,  0.0000000e+00,\n",
       "        1.9889315e+01,  2.8999142e+01,  0.0000000e+00,  2.2290781e+01,\n",
       "        0.0000000e+00,  8.9849998e+01,  0.0000000e+00,  5.1065273e+00,\n",
       "        0.0000000e+00,  1.1791352e+01,  2.2545193e+01,  1.0036171e+01,\n",
       "        6.3273201e+00,  1.6814537e+01,  1.9913841e+01,  3.0996582e+01,\n",
       "        5.7336674e+00,  2.5702536e+00,  3.3714914e+00,  8.3395090e+00,\n",
       "        1.5087716e+01,  8.1100502e+00,  5.3093350e-01,  5.2676330e+00,\n",
       "        1.4309413e+00,  4.0414710e+00,  0.0000000e+00,  2.1428572e-01,\n",
       "        2.1000000e+01,  3.0000000e+00,  6.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  1.0000000e+00,  2.0000000e+00,\n",
       "        3.0000000e+00,  6.0000000e+00,  2.0000000e+00,  7.0000000e+00,\n",
       "        2.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        3.0000000e+00,  3.1245000e+00,  8.4921204e+01,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  1.0000000e+00,  0.0000000e+00,\n",
       "        4.0000000e+00,  0.0000000e+00,  1.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  4.0000000e+00,  0.0000000e+00,\n",
       "        1.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        1.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  1.0000000e+00,  0.0000000e+00,  1.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  1.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        1.0000000e+00,  1.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "      dtype=float32))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task.samples[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Making a Task Sample\n",
    "\n",
    "In practice, all methods require that we sample from the `FSMolTask`s, for example during evaluation.\n",
    "To this end, we have implemented a number of samplers, extensions of the `TaskSampler` abstract class, which provides a unified `sample` method.\n",
    "The baseline models implemented in this repository use stratified sampling (implemented in `StratifiedTaskSampler`), but `RandomTaskSampler` and `BalancedTaskSampler` exist as alternatives.\n",
    "\n",
    "In practice, sampling often requires additional parameters, such as a minimal support set size, a minimal query set size, or an upper desired query set size. When a task dataset cannot be sampled using these parameters (e.g., because it's too small), a `SamplingException` exception is thrown and the caller can decide how to handle this. For example, during training, it is often reasonable to simply ignore such exceptions and pass over failure cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fs_mol.data import StratifiedTaskSampler\n",
    "task_sampler = StratifiedTaskSampler(\n",
    "    train_size_or_ratio = 16,\n",
    "    valid_size_or_ratio = 0.0,\n",
    "    test_size_or_ratio = 256, \n",
    "    allow_smaller_test = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5252f342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CHEMBL1243966'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying a sampler returns a sample from the task, which contains a support/train set, validation set, and query/test set. In this case, the task is too small to return all requested testing samples, so it returns the maximum available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in task: 201\n",
      "Number of train samples: 16\n",
      "Number of test samples: 185\n",
      "Number of valid samples: 0\n"
     ]
    }
   ],
   "source": [
    "task_sample = task_sampler.sample(task, seed=0)\n",
    "\n",
    "print(f\"Number of samples in task: {len(task.samples)}\")\n",
    "print(f\"Number of train samples: {len(task_sample.train_samples)}\")\n",
    "print(f\"Number of test samples: {len(task_sample.test_samples)}\")\n",
    "print(f\"Number of valid samples: {len(task_sample.valid_samples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The samplers are built such that using the same seed will always return the same data split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task sample, seed 0, first call  - First training SMILES Nc1ncnc2c1c(-c1ccc3[nH]ccc3c1)nn2C1CCCC1\n",
      "Task sample, seed 0, second call - First training SMILES Nc1ncnc2c1c(-c1ccc3[nH]ccc3c1)nn2C1CCCC1\n",
      "Task sample, seed 1, first call  - First training SMILES COc1cc(-c2nn(C(C)C)c3ncnc(N)c23)ccc1F\n"
     ]
    }
   ],
   "source": [
    "task_sample_0 = task_sampler.sample(task, seed=0)\n",
    "task_sample_1 = task_sampler.sample(task, seed=1)\n",
    "\n",
    "print(f\"Task sample, seed 0, first call  - First training SMILES {task_sample.train_samples[0].smiles}\")\n",
    "print(f\"Task sample, seed 0, second call - First training SMILES {task_sample_0.train_samples[0].smiles}\")\n",
    "print(f\"Task sample, seed 1, first call  - First training SMILES {task_sample_1.train_samples[0].smiles}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Custom Task Reading Functions in MAML\n",
    "\n",
    "When implementing models on top of FS-Mol, it is often useful to specialize the callback used in `get_task_reading_iterable`, for example to directly draw appropriate samples or do further featurization.\n",
    "\n",
    "In MAML, we for example need to use stratified samples in each training step:\n",
    "\n",
    "```python\n",
    "task_sampler = StratifiedTaskSampler(\n",
    "    train_size_or_ratio=train_size,\n",
    "    valid_size_or_ratio=0,\n",
    "    test_size_or_ratio=(min_test_size, test_size),\n",
    ")\n",
    "\n",
    "def read_and_sample_from_task(paths: List[RichPath], id: int) -> Iterable[FSMolTaskSample]:\n",
    "    for i, path in enumerate(paths):\n",
    "        task = FSMolTask.load_from_file(path)\n",
    "        yield task_sampler.sample(task, seed=id + i)\n",
    "\n",
    "train_task_samples = dataset.get_task_reading_iterable(\n",
    "    data_fold=DataFold.TRAIN, task_reader_fn=read_and_sample_from_task\n",
    ")\n",
    "```\n",
    "\n",
    "In this instance, `train_task_samples` will now be an `Iterable` of sampled tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching Task Samples\n",
    "\n",
    "The `fs_mol.data` package also provides infrastructure for minibatching, using the `FSMolBatcher` class.\n",
    "These are framework-agnostic, and are used both for TensorFlow (in our MAML baseline) and Torch (in our MAT and Multitask baselines).\n",
    "Concretely, an `FSMolBatcher` object can be used to turn a list of datapoints into a sequence of minibatches.\n",
    "Our graph models handle batches of graphs as a single graph in which the samples appear as disconnected components. This is already implemented by our default implementation of `FSMolBatcher`, and so consumers only need to provide thing extensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: GNN Multitask Batching\n",
    "\n",
    "As example, consider the GNN Multitask model (see `fs_mol/data/multitask.py` for full code), which needs to include the task ID for each sample. To do this, we extend the `FSMolBatcher` class using the callback hooks for initializing, extending and finalizing a batch:\n",
    "\n",
    "```python\n",
    "def multitask_batcher_init_fn(batch_data: Dict[str, Any]):\n",
    "    batch_data[\"sample_to_task_id\"] = []\n",
    "\n",
    "def multitask_batcher_add_sample_fn(\n",
    "    batch_data: Dict[str, Any],\n",
    "    sample_id: int,\n",
    "    sample: MoleculeDatapoint,\n",
    "    task_name_to_id: Dict[str, int],\n",
    "):\n",
    "    batch_data[\"sample_to_task_id\"].append(task_name_to_id[sample.task_name])\n",
    "\n",
    "def multitask_batcher_finalizer_fn(\n",
    "    batch_data: Dict[str, Any]\n",
    ") -> Tuple[FSMolMultitaskBatch, np.ndarray]:\n",
    "    plain_batch = fsmol_batch_finalizer(batch_data)\n",
    "    return (\n",
    "        FSMolMultitaskBatch(\n",
    "            sample_to_task_id=np.stack(batch_data[\"sample_to_task_id\"], axis=0),\n",
    "            **dataclasses.asdict(plain_batch),\n",
    "        ),\n",
    "        np.stack(batch_data[\"bool_labels\"], axis=0),\n",
    "    )\n",
    "\n",
    "def get_multitask_batcher(\n",
    "    task_name_to_id: Dict[str, int],\n",
    "    max_num_graphs: Optional[int] = None,\n",
    "    max_num_nodes: Optional[int] = None,\n",
    "    max_num_edges: Optional[int] = None,\n",
    ") -> FSMolBatcher[FSMolMultitaskBatch, np.ndarray]:\n",
    "    return FSMolBatcher(\n",
    "        max_num_graphs=max_num_graphs,\n",
    "        max_num_nodes=max_num_nodes,\n",
    "        max_num_edges=max_num_edges,\n",
    "        init_callback=multitask_batcher_init_fn,\n",
    "        per_datapoint_callback=partial(\n",
    "            multitask_batcher_add_sample_fn, task_name_to_id=task_name_to_id\n",
    "        ),\n",
    "        finalizer_callback=multitask_batcher_finalizer_fn,\n",
    "    )\n",
    "```\n",
    "\n",
    "This batcher is then used to create batches of up to `num_chunked_tasks` loaded in parallel as follows:\n",
    "\n",
    "```python\n",
    "def paths_to_mixed_samples(\n",
    "    paths: List[RichPath], idx: int\n",
    ") -> Iterable[Tuple[FSMolMultitaskBatch, np.ndarray]]:\n",
    "    loaded_samples: List[MoleculeDatapoint] = []\n",
    "    for i, path in enumerate(paths):\n",
    "        task = FSMolTask.load_from_file(path)\n",
    "        task_sample = self._task_sampler.sample(task, seed=idx + i)\n",
    "        loaded_samples.extend(task_sample.train_samples)\n",
    "    if self._data_fold == DataFold.TRAIN:\n",
    "        np.random.shuffle(loaded_samples)\n",
    "\n",
    "    for features, labels in self._batcher.batch(loaded_samples):\n",
    "        yield features, labels\n",
    "\n",
    "task_iterable = self._dataset.get_task_reading_iterable(\n",
    "    data_fold=self._data_fold,\n",
    "    task_reader_fn=paths_to_mixed_samples,\n",
    "    reader_chunk_size=self._num_chunked_tasks,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: MAML Batching\n",
    "\n",
    "We can similarly use this from a Tensorflow model, namely our MAML implementation (see `fs_mol/data/maml.py`). Here, we created a `TFGraphBatchIterable` class that uses a `FSMolBatcher` using a customized `finalizer_callback` to produce a batch suitable for consumption in our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-22 16:23:21.592810: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-22 16:23:21.672834: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-02-22 16:23:21.685894: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-02-22 16:23:24.570155: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-12.0/lib64\n",
      "2023-02-22 16:23:24.570999: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-12.0/lib64\n",
      "2023-02-22 16:23:24.571002: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'node_features': array([[0., 0., 1., ..., 1., 0., 0.],\n",
      "       [0., 1., 0., ..., 1., 0., 1.],\n",
      "       [0., 0., 1., ..., 1., 0., 1.],\n",
      "       ...,\n",
      "       [0., 1., 0., ..., 1., 0., 1.],\n",
      "       [0., 1., 0., ..., 1., 0., 1.],\n",
      "       [0., 0., 1., ..., 1., 0., 1.]], dtype=float32), 'node_to_graph_map': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], dtype=int32), 'num_graphs_in_batch': 3, 'adjacency_list_0': array([[ 0,  1],\n",
      "       [ 2,  3],\n",
      "       [ 4,  5],\n",
      "       [ 6,  7],\n",
      "       [ 7,  8],\n",
      "       [ 9, 10],\n",
      "       [11, 12],\n",
      "       [12, 13],\n",
      "       [14, 15],\n",
      "       [17, 18],\n",
      "       [18, 19],\n",
      "       [19, 20],\n",
      "       [20, 21],\n",
      "       [21, 22],\n",
      "       [22, 23],\n",
      "       [ 6,  1],\n",
      "       [16,  8],\n",
      "       [23, 19],\n",
      "       [18,  5],\n",
      "       [15, 11],\n",
      "       [24, 25],\n",
      "       [26, 27],\n",
      "       [27, 28],\n",
      "       [28, 29],\n",
      "       [30, 31],\n",
      "       [32, 33],\n",
      "       [33, 35],\n",
      "       [35, 36],\n",
      "       [37, 38],\n",
      "       [37, 39],\n",
      "       [40, 41],\n",
      "       [42, 43],\n",
      "       [32, 44],\n",
      "       [45, 46],\n",
      "       [46, 47],\n",
      "       [47, 48],\n",
      "       [48, 49],\n",
      "       [49, 50],\n",
      "       [50, 51],\n",
      "       [51, 52],\n",
      "       [52, 53],\n",
      "       [50, 54],\n",
      "       [54, 55],\n",
      "       [56, 25],\n",
      "       [44, 29],\n",
      "       [55, 47],\n",
      "       [42, 36],\n",
      "       [57, 58],\n",
      "       [59, 60],\n",
      "       [61, 62],\n",
      "       [63, 64],\n",
      "       [65, 66],\n",
      "       [66, 67],\n",
      "       [67, 68],\n",
      "       [67, 69],\n",
      "       [66, 70],\n",
      "       [71, 72],\n",
      "       [73, 74],\n",
      "       [74, 75],\n",
      "       [63, 77],\n",
      "       [78, 79],\n",
      "       [80, 58],\n",
      "       [79, 61],\n",
      "       [76, 64],\n",
      "       [76, 70]], dtype=int32), 'edge_features_0': array([], shape=(65, 0), dtype=float32), 'adjacency_list_1': array([[ 1,  2],\n",
      "       [ 3,  4],\n",
      "       [ 5,  6],\n",
      "       [ 8,  9],\n",
      "       [10, 11],\n",
      "       [13, 14],\n",
      "       [15, 16],\n",
      "       [ 7, 17],\n",
      "       [25, 26],\n",
      "       [29, 30],\n",
      "       [31, 32],\n",
      "       [33, 34],\n",
      "       [36, 37],\n",
      "       [39, 40],\n",
      "       [41, 42],\n",
      "       [27, 45],\n",
      "       [46, 56],\n",
      "       [58, 59],\n",
      "       [60, 61],\n",
      "       [62, 63],\n",
      "       [64, 65],\n",
      "       [70, 71],\n",
      "       [72, 73],\n",
      "       [74, 76],\n",
      "       [77, 78],\n",
      "       [79, 80]], dtype=int32), 'edge_features_1': array([], shape=(26, 0), dtype=float32), 'adjacency_list_2': array([], shape=(0, 2), dtype=int32), 'edge_features_2': array([], shape=(0, 0), dtype=float32)}, {'target_value': array([1., 0., 1.], dtype=float32)})\n"
     ]
    }
   ],
   "source": [
    "from fs_mol.data.maml import TFGraphBatchIterable\n",
    "batched_data = TFGraphBatchIterable(\n",
    "    samples=task_sample.train_samples,\n",
    "    shuffle=True,\n",
    "    max_num_nodes=100,\n",
    ")\n",
    "\n",
    "print(next(iter(batched_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14986ac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(batched_data))[1]['target_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9a109d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pytorch1.12_py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d5662b563e35f28d4014ff29caa1d66d3f415e3acef210ec3f93272ddd5f1dd8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
